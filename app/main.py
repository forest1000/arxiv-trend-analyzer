import streamlit as st
import asyncio
from arxiv_fetcher import fetch_arxiv_papers 

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="arXiv Trend Analyzer with Local LLM",
    page_icon="ğŸ”¬",
    layout="wide"
)

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("ğŸ”¬ ãƒ­ãƒ¼ã‚«ãƒ«LLMæ­è¼‰ arXivãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¢ãƒ—ãƒª")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€arXiv.orgã«æŠ•ç¨¿ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã€**ã‚ãªãŸã®PCã§å‹•ä½œã™ã‚‹LLMãŒå„è«–æ–‡ã®æ–°è¦æ€§ã‚’è¦ç´„**ã—ã¾ã™ã€‚
è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’**ã‚«ãƒ³ãƒï¼ˆ,ï¼‰**ã§åŒºåˆ‡ã£ã¦å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€ãã‚Œã‚‰å…¨ã¦ã‚’å«ã‚€è«–æ–‡ã‚’ANDæ¤œç´¢ã§ãã¾ã™ã€‚
""")
st.warning("**æ³¨æ„:** ã“ã®ã‚¢ãƒ—ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€äº‹å‰ã«Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")


# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
st.sidebar.header("æ¤œç´¢è¨­å®š")
# ANDæ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ (text_inputã‚’ä½¿ç”¨)
search_keywords_input = st.sidebar.text_input(
    "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ANDæ¤œç´¢)",
    'computer vision, object detection, transformer',
    help="""
    è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒï¼ˆ,ï¼‰ã§åŒºåˆ‡ã£ã¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n
    ä¾‹: `deep learning, medical imaging, segmentation`
    """
)
# å–å¾—ä»¶æ•°
max_results = st.sidebar.slider("æœ€å¤§å–å¾—ä»¶æ•°", 5, 100, 20)

# --- æ¤œç´¢å®Ÿè¡Œã¨çµæœè¡¨ç¤º ---
if st.sidebar.button("åˆ†æã‚’å®Ÿè¡Œ"):
    
    keywords = [k.strip() for k in search_keywords_input.split(',') if k.strip()]

    if not keywords:
        st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‹ã‚‰ `all:"keyword1" AND all:"keyword2"` å½¢å¼ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        final_query = " AND ".join([f'all:"{k}"' for k in keywords])
        st.sidebar.success("ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª:")
        st.sidebar.code(final_query, language='text')

        with st.spinner("è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«LLMã§æ–°è¦æ€§ã‚’åˆ†æã—ã¦ã„ã¾ã™... PCã®æ€§èƒ½ã«ã‚ˆã£ã¦ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚"):
            try:
                # ç”Ÿæˆã—ãŸå˜ä¸€ã®ã‚¯ã‚¨ãƒªã‚’ãƒªã‚¹ãƒˆã«å…¥ã‚Œã¦fetcherã«æ¸¡ã™
                papers = asyncio.run(fetch_arxiv_papers([final_query], max_results))
            except Exception as e:
                st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                papers = []

        if papers:
            st.success(f"{len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

            for i, paper in enumerate(papers):
                st.markdown("---")
                st.subheader(f"{i+1}. {paper['title']}")
                st.write(f"**è‘—è€…:** {', '.join(paper['authors'])}")
                st.write(f"**æŠ•ç¨¿æ—¥:** {paper['published_date']}")
                st.write(f"**arXivãƒªãƒ³ã‚¯:** [{paper['url']}]({paper['url']})")

                st.markdown("##### ğŸ¤– LLMã«ã‚ˆã‚‹æ–°è¦æ€§ã®è¦ç´„")
                st.info(paper['novelty'])

                with st.expander("å…ƒã®è¦æ—¨ï¼ˆAbstractï¼‰ã‚’èª­ã‚€"):
                    st.write(paper['summary'])
        else:
            st.error("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã€ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

async def output_paper_info(final_query, max_results):
    """
    A function to output paper information asynchronously.
    """
    st.markdown("---")
    st.subheader(paper['title'])
    st.write(f"**è‘—è€…:** {', '.join(paper['authors'])}")
    st.write(f"**æŠ•ç¨¿æ—¥:** {paper['published_date']}")
    st.write(f"**arXivãƒªãƒ³ã‚¯:** [{paper['url']}]({paper['url']})")

    st.markdown("##### ğŸ¤– LLMã«ã‚ˆã‚‹æ–°è¦æ€§ã®è¦ç´„")
    st.info(paper['novelty'])

    with st.expander("å…ƒã®è¦æ—¨ï¼ˆAbstractï¼‰ã‚’èª­ã‚€"):
        st.write(paper['summary'])