import asyncio

import streamlit as st

from arxiv_fetcher import NoveltyAnalyzer, fetch_arxiv_papers


st.set_page_config(
    page_title="arXiv Trend Analyzer with Local GPU LLM",
    page_icon="ğŸ”¬",
    layout="wide",
)

st.title("ğŸ”¬ GPUãƒ­ãƒ¼ã‚«ãƒ«è¦ç´„ä»˜ã arXivãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¢ãƒ—ãƒª")
st.markdown(
    """
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€arXiv.orgã«æŠ•ç¨¿ã•ã‚ŒãŸè«–æ–‡ã‚’æ¤œç´¢ã—ã€**ãƒ­ãƒ¼ã‚«ãƒ«GPUã§å‹•ä½œã™ã‚‹è¦ç´„ãƒ¢ãƒ‡ãƒ«ãŒå„è«–æ–‡ã®æ–°è¦æ€§ã‚’è¦ç´„**ã—ã¾ã™ã€‚
è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’**ã‚«ãƒ³ãƒï¼ˆ,ï¼‰**ã§åŒºåˆ‡ã£ã¦å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€ãã‚Œã‚‰å…¨ã¦ã‚’å«ã‚€è«–æ–‡ã‚’ANDæ¤œç´¢ã§ãã¾ã™ã€‚
"""
)
st.warning(
    "**æ³¨æ„:** åˆå›ã®æ¨è«–æ™‚ã«è¦ç´„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è‡ªå‹•çš„ã«ä½¿ç”¨ã—ã¾ã™ãŒã€CPUã§ã‚‚å‹•ä½œã—ã¾ã™ã€‚"
)

st.sidebar.header("æ¤œç´¢è¨­å®š")
search_keywords_input = st.sidebar.text_input(
    "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ANDæ¤œç´¢)",
    "computer vision, object detection, transformer",
    help="""
    è¤‡æ•°ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ³ãƒï¼ˆ,ï¼‰ã§åŒºåˆ‡ã£ã¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n
    ä¾‹: `deep learning, medical imaging, segmentation`
    """,
)
max_results = st.sidebar.slider("æœ€å¤§å–å¾—ä»¶æ•°", 5, 100, 20)

novelty_analyzer = NoveltyAnalyzer()


def _run_fetch(query: str, limit: int):
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(
            fetch_arxiv_papers(query, limit, novelty_analyzer=novelty_analyzer)
        )
    finally:
        loop.run_until_complete(asyncio.sleep(0))
        loop.close()
        asyncio.set_event_loop(None)


if st.sidebar.button("åˆ†æã‚’å®Ÿè¡Œ"):
    keywords = [k.strip() for k in search_keywords_input.split(",") if k.strip()]

    if not keywords:
        st.warning("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        final_query = " AND ".join([f'all:"{k}"' for k in keywords])
        st.sidebar.success("ç”Ÿæˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª:")
        st.sidebar.code(final_query, language="text")

        with st.spinner(
            "è«–æ–‡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€GPUè¦ç´„ãƒ¢ãƒ‡ãƒ«ã§æ–°è¦æ€§ã‚’åˆ†æã—ã¦ã„ã¾ã™... PCã®æ€§èƒ½ã«ã‚ˆã£ã¦ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚"
        ):
            try:
                papers = _run_fetch(final_query, max_results)
            except Exception as exc:  # pragma: no cover - runtime guard for UI usage
                st.error(f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}")
                papers = []

        if papers:
            st.success(f"{len(papers)}ä»¶ã®è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

            for i, paper in enumerate(papers):
                st.markdown("---")
                st.subheader(f"{i + 1}. {paper['title']}")
                st.write(f"**è‘—è€…:** {', '.join(paper['authors'])}")
                st.write(f"**æŠ•ç¨¿æ—¥:** {paper['published_date']}")
                st.write(f"**arXivãƒªãƒ³ã‚¯:** [{paper['url']}]({paper['url']})")

                st.markdown("##### ğŸ¤– è¦ç´„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–°è¦æ€§ã®è¦ç´„")
                st.info(paper["novelty"])

                with st.expander("å…ƒã®è¦æ—¨ï¼ˆAbstractï¼‰ã‚’èª­ã‚€"):
                    st.write(paper["summary"])
        else:
            st.error("è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
else:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã€ã€Œåˆ†æã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
